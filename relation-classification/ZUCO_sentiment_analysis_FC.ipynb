{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing steps\n",
    "## In Jupyter \n",
    "\n",
    "1. load ALL subject's data (pkl)\n",
    "2. set fb, cc, fs, step, estimator\n",
    "3. seperate Pos, Neutral, Neg or Pos, Neg\n",
    "3. fcg calculate (all subject & all words of sentences & all fixation)\n",
    "4. fcg stack\n",
    "5. fcgs -> neural gas fit (-> mdl // encoding, symbols)\n",
    "6. mdl save /// \n",
    "\n",
    "## In File \n",
    "7. load mdl and set property\n",
    "8. load eeg & transpose EEG\n",
    "9. calculate fcg\n",
    "10. using mdl, encode fcg to brain_state\n",
    "\n",
    "## Plus In Jupyter\n",
    "10. encode fcgs to brain_state\n",
    "11. Neg vs Pos result check (histogram, olotmatrix, markov_matrix, occupancy rate, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "import scipy\n",
    "from dyconnmap.fc import iplv\n",
    "import matplotlib.pyplot as plt\n",
    "from dyconnmap import tvfcg\n",
    "from dyconnmap.fc import IPLV\n",
    "from dyconnmap.ts import transition_rate, occupancy_time\n",
    "from dyconnmap.cluster import NeuralGas\n",
    "import tqdm\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All subject's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SR data\n",
    "\n",
    "mat_file = []\n",
    "subject = []\n",
    "\n",
    "#rootdir = '/home/KimDY/zuco-nlp/sentiment-analysis/Data_to_preprocess/Sentence_label_data/'\n",
    "rootdir = \"C:/Users/김다예/OneDrive - 경희대학교/바탕 화면/연구/ZUCO/REAL/data/\"\n",
    "sentences = []\n",
    "subject = []\n",
    "\n",
    "for file in os.listdir(rootdir):\n",
    "    if file.endswith(\".pickle\"):\n",
    "        print(\"파일명 : \",file)\n",
    "        subject.append(file.split(\"_\")[2].split(\".\")[0])\n",
    "        print(\"피험자 : \",file.split(\"_\")[2].split(\".\")[0])\n",
    "\n",
    "        file_name = rootdir + file\n",
    "        # file load\n",
    "        with open(file_name, \"rb\") as f:\n",
    "            mat_file.append(pickle.load(f)) #저장 및 불러오기 완료\n",
    "\n",
    "print(\"...done!🎉\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. set property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. seperate Pos, Neutral, Neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to WORD LEVEL analysis and fit? \n",
    "# or Sentence LEVEL analysis and fit... << first Do\n",
    "# for later word level encode... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file[0][0].keys() #0 subject, 0sentence\n",
    "mat_file[0][3][\"RAW_EEG\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file[0][1][\"label\"]\n",
    "mat_file[0][2][\"label_name\"]\n",
    "print(len(mat_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_dict = []\n",
    "NEU_dict = []\n",
    "NEG_dict = []\n",
    "\n",
    "for i in range(len(subject)):\n",
    "    check_file = mat_file[i]\n",
    "    for j in range(len(check_file)):\n",
    "        label = check_file[j][\"label_name\"]\n",
    "        if (label == 'POSITIVE'):\n",
    "            POS_dict.append(mat_file[i][j])\n",
    "        elif (label == 'NEUTRAL'):\n",
    "            NEU_dict.append(mat_file[i][j])\n",
    "        elif (label == 'NEGATIVE'):\n",
    "            NEG_dict.append(mat_file[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_eeg = []\n",
    "NEU_eeg = []\n",
    "NEG_eeg = []\n",
    "\n",
    "for i in range(len(POS_dict)):\n",
    "    POS_eeg.append(POS_dict[i][\"RAW_EEG\"].T)\n",
    "    \n",
    "for j in range(len(NEU_dict)):\n",
    "    NEU_eeg.append(NEU_dict[j][\"RAW_EEG\"].T)\n",
    "    \n",
    "for k in range(len(NEG_dict)):\n",
    "    NEG_eeg.append(NEG_dict[k][\"RAW_EEG\"].T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(POS_dict))\n",
    "print(len(NEU_dict))\n",
    "print(len(NEG_dict))\n",
    "\n",
    "\n",
    "print(len(POS_eeg))\n",
    "print(len(NEU_eeg))\n",
    "print(len(NEG_eeg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. fcg calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to find more information about cc, fs etc. ex> equally seperate?\n",
    "\n",
    "mat_file[0][0][\"word_level_data\"].keys()\n",
    "mat_file[0][0][\"word_level_data\"][1] # NOTE! 1 to max (not 0)\n",
    "mat_file[0][0][\"word_level_data\"][1].keys()\n",
    "\n",
    "# max_sentence_length = len(mat_file[0])\n",
    "# max_word_length = len(mat_file[0][0][\"word_level_data\"]) - 1\n",
    "# because, It has 1 additional 'word_reading_order' key.\n",
    "\n",
    "# mat_file[sub][sen].keys()\n",
    "\"\"\"\n",
    "'RAW_EEG', 'ICA_EEG', 'content', 'sentence_number', 'label', \n",
    "'word_embedding_idxs', 'label_content', 'label_name', 'word_level_data'\n",
    "\"\"\"\n",
    "\n",
    "# mat_file[sub][sen][\"word_level_data\"][word].keys()\n",
    "\"\"\"\n",
    "['RAW_EEG', 'ICA_EEG', 'RAW_ET', 'FFD', 'GD', 'GPT', \n",
    "'TRT', 'nFix', 'word_idx', 'content']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence_fcgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "theta1 (4-6Hz), theta2 (6.5-8 Hz), alpha1 (8.5-10 Hz), alpha2(10.5-13 Hz), \n",
    "beta1, (13.5-18 Hz) beta2 (18.5-30Hz), \n",
    "gamma1 (30.5-40 Hz) and gamma2 (40-49.5Hz) \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb = [4.0, 50.0] #lowest , Highest frequency band #band 별로 각각 살펴봐야하나?\n",
    "cc = 4.0 #what is cycle-criterion ?????????\n",
    "fs = 500.0 #sampling frequency\n",
    "step = 250 #amount of sample that caculate once (window)\n",
    "\n",
    "estimator = IPLV(fb, fs)\n",
    "# need to (channel,time)\n",
    "\n",
    "\"\"\"\n",
    "tutorial sample : \n",
    "\n",
    "fb = [7.0, 13.0]\n",
    "cc = 4.0\n",
    "fs = 160.0\n",
    "step = 80 #this time, we will slice 80 for window \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "print(POS_eeg[0].shape)\n",
    "print(len(POS_dict[0][\"word_level_data\"])-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS\n",
    "i = 0\n",
    "pos_err = 0\n",
    "### 1_sentence >>\n",
    "\n",
    "first_fix_eeg = POS_eeg[0]\n",
    "fcg = tvfcg(np.squeeze(first_fix_eeg), estimator, fb, fs, cc, step) #1_word_1_fix\n",
    "POS_fcgs = np.array(np.real(fcg))\n",
    "\n",
    "#### and else >> \n",
    "# len(POS_eeg)\n",
    "for i in tqdm.tqdm(range(1, len(POS_eeg))):\n",
    "    try: \n",
    "        data = POS_eeg[i]          \n",
    "        fcg = tvfcg(np.squeeze(data), estimator, fb, fs, cc, step)\n",
    "        POS_fcgs = np.vstack([POS_fcgs,np.array(np.real(fcg))])\n",
    "\n",
    "    except:\n",
    "        print(\"window is too large for this sentence\")\n",
    "        pos_err +=1\n",
    "\n",
    "print(\"...done!🎉\")   \n",
    "print(\"POS_fcgs = \", len(POS_fcgs))\n",
    "print(\"Error sentence =\", pos_err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos_err)\n",
    "POS_fcgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEU\n",
    "neu_err = 0\n",
    "### 1_sentence >>\n",
    "first_fix_eeg = NEU_eeg[0]\n",
    "fcg = tvfcg(np.squeeze(first_fix_eeg), estimator, fb, fs, cc, step) #1_word_1_fix\n",
    "NEU_fcgs = np.array(np.real(fcg))\n",
    "\n",
    "#### and else >> \n",
    "for i in tqdm.tqdm(range(1, len(NEU_eeg))):\n",
    "    try:\n",
    "        data = NEU_eeg[i]          \n",
    "        fcg = tvfcg(np.squeeze(data), estimator, fb, fs, cc, step)\n",
    "        NEU_fcgs = np.vstack([NEU_fcgs,np.array(np.real(fcg))])\n",
    "        \n",
    "    except:\n",
    "        print(\"window is too large for this sentence\")\n",
    "        neu_err +=1\n",
    "    \n",
    "print(\"...done!🎉\")   \n",
    "print(\"NEU_fcgs = \", len(NEU_fcgs))\n",
    "print(\"Error sentence =\", neu_err)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEG\n",
    "neg_err = 0\n",
    "\n",
    "### 1_sentence >>\n",
    "first_fix_eeg = NEG_eeg[0]\n",
    "fcg = tvfcg(np.squeeze(first_fix_eeg), estimator, fb, fs, cc, step) #1_word_1_fix\n",
    "NEG_fcgs = np.array(np.real(fcg))\n",
    "\n",
    "#### and else >> \n",
    "#len(NEG_eeg)\n",
    "for i in tqdm.tqdm(range(1, len(NEG_eeg))):\n",
    "    try:\n",
    "        data = NEG_eeg[i]         \n",
    "        fcg = tvfcg(np.squeeze(data), estimator, fb, fs, cc, step)\n",
    "        NEG_fcgs = np.vstack([NEG_fcgs,np.array(np.real(fcg))])\n",
    "\n",
    "    except:\n",
    "        print(\"window is too large for this sentence\")\n",
    "        neg_err +=1\n",
    "\n",
    "\n",
    "print(\"...done!🎉\")   \n",
    "print(\"NEG_fcgs = \", len(NEG_fcgs))\n",
    "print(\"Error sentence =\", neg_err)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(POS_fcgs.shape)\n",
    "#print(NEU_fcgs.shape)\n",
    "print(NEG_fcgs.shape)\n",
    "\n",
    "print(pos_err)\n",
    "#print(neu_err)\n",
    "print(neg_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P/N/N dict\n",
    "#P/N/N fcgs\n",
    "\n",
    "#save_dir = \"/home/KimDY/zuco-nlp/sentiment-analysis/Data_to_preprocess/\"\n",
    "\n",
    "with open(\"POS_theta_fcgs.pkl\", \"wb\") as f:\n",
    "   pickle.dump(POS_fcgs, f)\n",
    "# with open(\"NEU_fcgs.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(NEU_fcgs, f)\n",
    "with open(\"NEG_theta_fcgs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(NEG_fcgs, f)\n",
    "    \n",
    "# with open(\"POS_dict.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(POS_dict, f)\n",
    "# with open(\"NEU_dict.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(NEU_dict, f)\n",
    "# with open(\"NEG_dict.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(NEG_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "import scipy\n",
    "from dyconnmap.fc import iplv\n",
    "import matplotlib.pyplot as plt\n",
    "from dyconnmap import tvfcg\n",
    "from dyconnmap.fc import IPLV\n",
    "from dyconnmap.ts import transition_rate, occupancy_time\n",
    "from dyconnmap.cluster import NeuralGas\n",
    "import tqdm\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NEG_fcgs.pkl\", \"rb\") as f:\n",
    "    NEG_fcgs = pickle.load(f)\n",
    "    \n",
    "with open(\"POS_fcgs.pkl\", \"rb\") as f:\n",
    "    POS_fcgs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_fcgs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. fcg stack and Neural gas fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 class (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fcgs_POS, _, _ = np.shape(POS_fcgs)\n",
    "num_fcgs_NEG, _, _ = np.shape(NEG_fcgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(POS_fcgs.shape)\n",
    "print(NEG_fcgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcgs = np.vstack([POS_fcgs, NEG_fcgs])\n",
    "print(len(fcgs))\n",
    "num_fcgs, num_channels, num_channels = np.shape(fcgs)\n",
    "\n",
    "triu_ind = np.triu_indices_from(np.squeeze(fcgs[0, ...]), k=1)\n",
    "\n",
    "fcgs = fcgs[:, triu_ind[0], triu_ind[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "# what \"N\" is best?\n",
    "mdl = NeuralGas(n_protos=10, rng=rng).fit(fcgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding, symbols = mdl.encode(fcgs)\n",
    "\n",
    "#Separate the encoded symbols based on their original groupings\n",
    "grp_dist_POS = symbols[0:num_fcgs_POS]\n",
    "grp_dist_NEG = symbols[num_fcgs_POS:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "with open(\"2class_mdl.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mdl, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHECK BEFORE INSERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0. import \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "import scipy\n",
    "from dyconnmap.fc import iplv\n",
    "import matplotlib.pyplot as plt\n",
    "from dyconnmap import tvfcg\n",
    "from dyconnmap.fc import IPLV\n",
    "from dyconnmap.ts import transition_rate, occupancy_time\n",
    "from dyconnmap.cluster import NeuralGas\n",
    "import tqdm\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. load mdl & set property\n",
    "with open(\"2class_mdl.pkl\", \"rb\") as f:\n",
    "    mdl = pickle.load(f)\n",
    "    \n",
    "fb = [4.0, 50.0] #lowest , Highest frequency band\n",
    "cc = 4.0\n",
    "fs = 500.0\n",
    "step = 250 #this time, we will slice 80 for window \n",
    "\n",
    "\n",
    "\n",
    "estimator = IPLV(fb, fs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. load EEG & Transpose EEG\n",
    "with open(\"POS_dict.pkl\", \"rb\") as f:\n",
    "    POS_dict = pickle.load(f)    \n",
    "POS_eeg = []\n",
    "\n",
    "for i in range(len(POS_dict)):\n",
    "    POS_eeg.append(POS_dict[i][\"RAW_EEG\"].T) #need to transpose\n",
    "\n",
    "#POS_eeg[0].shape == (105, 3412)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 105, 105)\n",
      "(12, 5460)\n"
     ]
    }
   ],
   "source": [
    "#9. calculate fcg\n",
    "eeg = POS_eeg[0] #1 개의 EEG \n",
    "fcg = tvfcg(np.squeeze(eeg), estimator, fb, fs, cc, step) #1_word_1_fix\n",
    "fcgs = np.array(np.real(fcg))\n",
    "\n",
    "print(fcgs.shape)\n",
    "#fcgs.shape = (12, 105, 105) <- 현재 피클파일로 저장되어있는 fcg 형태. 문장에 상관없이 그냥 몽땅 Stack 되어있음. 앞으로 쓸일없을듯? \n",
    "\n",
    "triu_ind = np.triu_indices_from(np.squeeze(fcgs[0, ...]), k=1)\n",
    "fcgs = fcgs[:, triu_ind[0], triu_ind[1]]\n",
    "\n",
    "print(fcgs.shape)\n",
    "\n",
    "#fcgs.shape == (12, 5460) <- encode 전에 넣어야 하는 fcg 형태 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. Using mdl, encode \n",
    "encoding, symbols = mdl.encode(fcgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 1)\n",
      "(12, 1, 5460)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.06264571, 0.07973764, 0.09829245, ..., 0.04474551,\n",
       "         0.02467856, 0.03130361]],\n",
       "\n",
       "       [[0.04397912, 0.04797952, 0.05892048, ..., 0.04241011,\n",
       "         0.02207803, 0.02168266]],\n",
       "\n",
       "       [[0.04397912, 0.04797952, 0.05892048, ..., 0.04241011,\n",
       "         0.02207803, 0.02168266]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.05965754, 0.07291722, 0.11318607, ..., 0.04981235,\n",
       "         0.02560415, 0.02311476]],\n",
       "\n",
       "       [[0.05325136, 0.06283716, 0.06421158, ..., 0.03514461,\n",
       "         0.02386462, 0.02417941]],\n",
       "\n",
       "       [[0.06264571, 0.07973764, 0.09829245, ..., 0.04474551,\n",
       "         0.02467856, 0.03130361]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(symbols.shape) #일단 이게 훈련 모델에 들어감...\n",
    "print(encoding.shape)\n",
    "\n",
    "symbols #fcg 12장 각각 1개씩 있는 brain state (label, int) 정보\n",
    "encoding # fcg 12장 각각에 1개씩 있는 brain state soft label 정보. 이때 soft label은 각 채널(??)마다 있음.\n",
    "\n",
    "# 고민1. symbols 을 넣을지 encoding 을 넣을지?\n",
    "# 고민2. 어떠한 형태로 넣어줄지?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLUS analysis IN Jupyter_2Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_dist_POS = symbols[0:num_fcgs_POS]\n",
    "grp_dist_NEG = symbols[num_fcgs_POS:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_grp_dist_POS = np.histogram(grp_dist_POS, bins=mdl.n_protos, normed=True)\n",
    "h_grp_dist_NEG = np.histogram(grp_dist_NEG, bins=mdl.n_protos, normed=True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ind = np.arange(mdl.n_protos)\n",
    "p1 = ax.bar(ind - 0.125, h_grp_dist_POS[0], 0.25, label='POS')\n",
    "p2 = ax.bar(ind + 0.125, h_grp_dist_NEG[0], 0.25, label='NEG')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('Symbol Index')\n",
    "ax.set_ylabel('Hits %')\n",
    "ax.set_xticks(np.arange(mdl.n_protos))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protos_mtx = np.zeros((mdl.n_protos, 105, 105))\n",
    "\n",
    "for i in range(mdl.n_protos):\n",
    "    symbol_state = np.zeros((105, 105))\n",
    "    symbol_state[triu_ind] = mdl.protos[i, :]\n",
    "    symbol_state = symbol_state + symbol_state.T\n",
    "    np.fill_diagonal(symbol_state, 1.0)\n",
    "    \n",
    "    protos_mtx[i, :, :] = symbol_state\n",
    "    \n",
    "mtx_min = np.min(protos_mtx)\n",
    "mtx_max = np.max(protos_mtx)\n",
    "\n",
    "f, ax = plt.subplots(ncols=mdl.n_protos, figsize=(12, 12))\n",
    "for i in range(mdl.n_protos):\n",
    "    cax = ax[i].imshow(np.squeeze(protos_mtx[i,...]), vmin=mtx_min, vmax=mtx_max, cmap=plt.cm.Spectral)\n",
    "    ax[i].set_title('#{0}'.format(i))\n",
    "\n",
    "# move the colorbar to the side ;)\n",
    "f.subplots_adjust(right=0.8)\n",
    "cbar_ax = f.add_axes([0.82, 0.445, 0.0125, 0.115])\n",
    "cb = f.colorbar(cax, cax=cbar_ax)\n",
    "cb.set_label('Imaginary PLV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grp_dist_POS.shape)\n",
    "print(grp_dist_NEG.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine the first subject\n",
    "\n",
    "subj1_POS = grp_dist_POS\n",
    "subj1_NEG = grp_dist_NEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dyconnmap.ts import markov_matrix\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "markov_matrix_eo = markov_matrix(subj1_POS)\n",
    "markov_matrix_ec = markov_matrix(subj1_NEG)\n",
    "\n",
    "f = plt.figure(figsize=(8, 6))\n",
    "grid = ImageGrid(f, 111,\n",
    "                 nrows_ncols=(1,2),\n",
    "                 axes_pad=0.15,\n",
    "                 share_all=True,\n",
    "                 cbar_location=\"right\",\n",
    "                 cbar_mode=\"single\",\n",
    "                 cbar_size=\"7%\",\n",
    "                 cbar_pad=0.15,\n",
    "                 )\n",
    "im = grid[0].imshow(markov_matrix_eo, vmin=0.0, vmax=1.0, cmap=plt.cm.Spectral)\n",
    "grid[0].set_xlabel('Prototype')\n",
    "grid[0].set_ylabel('Prototype')\n",
    "grid[0].set_title('NR')\n",
    "\n",
    "im = grid[1].imshow(markov_matrix_ec, vmin=0.0, vmax=1.0, cmap=plt.cm.Spectral)\n",
    "grid[1].set_xlabel('Prototype')\n",
    "grid[1].set_ylabel('Prototype')\n",
    "grid[1].set_title('TSR')\n",
    "\n",
    "cb = grid[1].cax.colorbar(im)\n",
    "cax = grid.cbar_axes[0]\n",
    "axis = cax.axis[cax.orientation]\n",
    "axis.label.set_text(\"Transition Probability\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dyconnmap.ts import transition_rate, occupancy_time\n",
    "\n",
    "\n",
    "tr_eo = transition_rate(subj1_POS)\n",
    "tr_ec = transition_rate(subj1_NEG)\n",
    "\n",
    "print(f\"\"\"\n",
    "Transition rate\n",
    "===============\n",
    "  NR: {tr_eo:.3f}\n",
    "  TSR : {tr_ec:.3f}\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_eo = occupancy_time(subj1_POS)[0]\n",
    "occ_ec = occupancy_time(subj1_NEG)[0]\n",
    "\n",
    "print(\"\"\"\n",
    "Occupancy time\n",
    "==============\n",
    "    State \\t 0 \\t 1 \\t 2 \\t 3 \\t 4\n",
    "      -----\n",
    "  NR:      \\t {0:.3f} \\t {1:.3f} \\t {2:.3f} \\t {3:.3f} \\t {4:.3f}\n",
    "  TSR : \\t {5:.3f} \\t {6:.3f} \\t {7:.3f} \\t {8:.3f} \\t {9:.3f}\n",
    "\"\"\".format(*occ_eo, *occ_ec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fcgs_NEU, _, _ = np.shape(NEU_fcgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcgs = np.vstack([POS_fcgs, NEU_fcgs, NEG_fcgs]) #can?\n",
    "print(len(fcgs))\n",
    "num_fcgs, num_channels, num_channels = np.shape(fcgs)\n",
    "\n",
    "triu_ind = np.triu_indices_from(np.squeeze(fcgs[0, ...]), k=1)\n",
    "\n",
    "fcgs = fcgs[:, triu_ind[0], triu_ind[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "# what \"N\" is best?\n",
    "mdl = NeuralGas(n_protos=5, rng=rng).fit(fcgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "with open(save_dir + \"3class_mdl.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mdl, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding, symbols = mdl.encode(fcgs)\n",
    "\n",
    "#Separate the encoded symbols based on their original groupings\n",
    "grp_dist_POS = symbols[0:num_fcgs_POS]\n",
    "grp_dist_NEU = symbols[num_fcgs_POS:num_fcgs_NEU]\n",
    "grp_dist_NEG = symbols[num_fcgs_NEU:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLUS analysis IN Jupyter_3Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
