{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing steps\n",
    "## In Jupyter \n",
    "\n",
    "1. load ALL subject's data (pkl)\n",
    "2. set fb, cc, fs, step, estimator\n",
    "3. seperate Pos, Neutral, Neg or Pos, Neg\n",
    "3. fcg calculate (all subject & all words of sentences & all fixation)\n",
    "4. fcg stack\n",
    "5. fcgs -> neural gas fit (-> mdl // encoding, symbols)\n",
    "6. mdl save /// \n",
    "\n",
    "## In File \n",
    "7. load mdl and set property\n",
    "8. load eeg & transpose EEG\n",
    "9. calculate fcg\n",
    "10. using mdl, encode fcg to brain_state\n",
    "\n",
    "## Plus In Jupyter\n",
    "10. encode fcgs to brain_state\n",
    "11. Neg vs Pos result check (histogram, olotmatrix, markov_matrix, occupancy rate, etc.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "import scipy\n",
    "from dyconnmap.fc import iplv\n",
    "import matplotlib.pyplot as plt\n",
    "from dyconnmap import tvfcg\n",
    "from dyconnmap.fc import IPLV\n",
    "from dyconnmap.ts import transition_rate, occupancy_time\n",
    "from dyconnmap.cluster import NeuralGas\n",
    "import tqdm\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load All subject's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get SR data\n",
    "\n",
    "mat_file = []\n",
    "subject = []\n",
    "\n",
    "#rootdir = '/home/KimDY/zuco-nlp/sentiment-analysis/Data_to_preprocess/Sentence_label_data/'\n",
    "rootdir = \"C:/Users/ÍπÄÎã§Ïòà/OneDrive - Í≤ΩÌù¨ÎåÄÌïôÍµê/Î∞îÌÉï ÌôîÎ©¥/Ïó∞Íµ¨/ZUCO/REAL/data/\"\n",
    "sentences = []\n",
    "subject = []\n",
    "\n",
    "for file in os.listdir(rootdir):\n",
    "    if file.endswith(\".pickle\"):\n",
    "        print(\"ÌååÏùºÎ™Ö : \",file)\n",
    "        subject.append(file.split(\"_\")[2].split(\".\")[0])\n",
    "        print(\"ÌîºÌóòÏûê : \",file.split(\"_\")[2].split(\".\")[0])\n",
    "\n",
    "        file_name = rootdir + file\n",
    "        # file load\n",
    "        with open(file_name, \"rb\") as f:\n",
    "            mat_file.append(pickle.load(f)) #Ï†ÄÏû• Î∞è Î∂àÎü¨Ïò§Í∏∞ ÏôÑÎ£å\n",
    "\n",
    "print(\"...done!üéâ\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. set property"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. seperate Pos, Neutral, Neg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to WORD LEVEL analysis and fit? \n",
    "# or Sentence LEVEL analysis and fit... << first Do\n",
    "# for later word level encode... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file[0][0].keys() #0 subject, 0sentence\n",
    "mat_file[0][3][\"RAW_EEG\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_file[0][1][\"label\"]\n",
    "mat_file[0][2][\"label_name\"]\n",
    "print(len(mat_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_dict = []\n",
    "NEU_dict = []\n",
    "NEG_dict = []\n",
    "\n",
    "for i in range(len(subject)):\n",
    "    check_file = mat_file[i]\n",
    "    for j in range(len(check_file)):\n",
    "        label = check_file[j][\"label_name\"]\n",
    "        if (label == 'POSITIVE'):\n",
    "            POS_dict.append(mat_file[i][j])\n",
    "        elif (label == 'NEUTRAL'):\n",
    "            NEU_dict.append(mat_file[i][j])\n",
    "        elif (label == 'NEGATIVE'):\n",
    "            NEG_dict.append(mat_file[i][j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_eeg = []\n",
    "NEU_eeg = []\n",
    "NEG_eeg = []\n",
    "\n",
    "for i in range(len(POS_dict)):\n",
    "    POS_eeg.append(POS_dict[i][\"RAW_EEG\"].T)\n",
    "    \n",
    "for j in range(len(NEU_dict)):\n",
    "    NEU_eeg.append(NEU_dict[j][\"RAW_EEG\"].T)\n",
    "    \n",
    "for k in range(len(NEG_dict)):\n",
    "    NEG_eeg.append(NEG_dict[k][\"RAW_EEG\"].T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(POS_dict))\n",
    "print(len(NEU_dict))\n",
    "print(len(NEG_dict))\n",
    "\n",
    "\n",
    "print(len(POS_eeg))\n",
    "print(len(NEU_eeg))\n",
    "print(len(NEG_eeg))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. fcg calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to find more information about cc, fs etc. ex> equally seperate?\n",
    "\n",
    "mat_file[0][0][\"word_level_data\"].keys()\n",
    "mat_file[0][0][\"word_level_data\"][1] # NOTE! 1 to max (not 0)\n",
    "mat_file[0][0][\"word_level_data\"][1].keys()\n",
    "\n",
    "# max_sentence_length = len(mat_file[0])\n",
    "# max_word_length = len(mat_file[0][0][\"word_level_data\"]) - 1\n",
    "# because, It has 1 additional 'word_reading_order' key.\n",
    "\n",
    "# mat_file[sub][sen].keys()\n",
    "\"\"\"\n",
    "'RAW_EEG', 'ICA_EEG', 'content', 'sentence_number', 'label', \n",
    "'word_embedding_idxs', 'label_content', 'label_name', 'word_level_data'\n",
    "\"\"\"\n",
    "\n",
    "# mat_file[sub][sen][\"word_level_data\"][word].keys()\n",
    "\"\"\"\n",
    "['RAW_EEG', 'ICA_EEG', 'RAW_ET', 'FFD', 'GD', 'GPT', \n",
    "'TRT', 'nFix', 'word_idx', 'content']\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentence_fcgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "theta1 (4-6Hz), theta2 (6.5-8 Hz), alpha1 (8.5-10 Hz), alpha2(10.5-13 Hz), \n",
    "beta1, (13.5-18 Hz) beta2 (18.5-30Hz), \n",
    "gamma1 (30.5-40 Hz) and gamma2 (40-49.5Hz) \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fb = [4.0, 50.0] #lowest , Highest frequency band #band Î≥ÑÎ°ú Í∞ÅÍ∞Å ÏÇ¥Ìé¥Î¥êÏïºÌïòÎÇò?\n",
    "cc = 4.0 #what is cycle-criterion ?????????\n",
    "fs = 500.0 #sampling frequency\n",
    "step = 250 #amount of sample that caculate once (window)\n",
    "\n",
    "estimator = IPLV(fb, fs)\n",
    "# need to (channel,time)\n",
    "\n",
    "\"\"\"\n",
    "tutorial sample : \n",
    "\n",
    "fb = [7.0, 13.0]\n",
    "cc = 4.0\n",
    "fs = 160.0\n",
    "step = 80 #this time, we will slice 80 for window \n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "print(POS_eeg[0].shape)\n",
    "print(len(POS_dict[0][\"word_level_data\"])-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS\n",
    "i = 0\n",
    "pos_err = 0\n",
    "### 1_sentence >>\n",
    "\n",
    "first_fix_eeg = POS_eeg[0]\n",
    "fcg = tvfcg(np.squeeze(first_fix_eeg), estimator, fb, fs, cc, step) #1_word_1_fix\n",
    "POS_fcgs = np.array(np.real(fcg))\n",
    "\n",
    "#### and else >> \n",
    "# len(POS_eeg)\n",
    "for i in tqdm.tqdm(range(1, len(POS_eeg))):\n",
    "    try: \n",
    "        data = POS_eeg[i]          \n",
    "        fcg = tvfcg(np.squeeze(data), estimator, fb, fs, cc, step)\n",
    "        POS_fcgs = np.vstack([POS_fcgs,np.array(np.real(fcg))])\n",
    "\n",
    "    except:\n",
    "        print(\"window is too large for this sentence\")\n",
    "        pos_err +=1\n",
    "\n",
    "print(\"...done!üéâ\")   \n",
    "print(\"POS_fcgs = \", len(POS_fcgs))\n",
    "print(\"Error sentence =\", pos_err)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos_err)\n",
    "POS_fcgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEU\n",
    "neu_err = 0\n",
    "### 1_sentence >>\n",
    "first_fix_eeg = NEU_eeg[0]\n",
    "fcg = tvfcg(np.squeeze(first_fix_eeg), estimator, fb, fs, cc, step) #1_word_1_fix\n",
    "NEU_fcgs = np.array(np.real(fcg))\n",
    "\n",
    "#### and else >> \n",
    "for i in tqdm.tqdm(range(1, len(NEU_eeg))):\n",
    "    try:\n",
    "        data = NEU_eeg[i]          \n",
    "        fcg = tvfcg(np.squeeze(data), estimator, fb, fs, cc, step)\n",
    "        NEU_fcgs = np.vstack([NEU_fcgs,np.array(np.real(fcg))])\n",
    "        \n",
    "    except:\n",
    "        print(\"window is too large for this sentence\")\n",
    "        neu_err +=1\n",
    "    \n",
    "print(\"...done!üéâ\")   \n",
    "print(\"NEU_fcgs = \", len(NEU_fcgs))\n",
    "print(\"Error sentence =\", neu_err)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEG\n",
    "neg_err = 0\n",
    "\n",
    "### 1_sentence >>\n",
    "first_fix_eeg = NEG_eeg[0]\n",
    "fcg = tvfcg(np.squeeze(first_fix_eeg), estimator, fb, fs, cc, step) #1_word_1_fix\n",
    "NEG_fcgs = np.array(np.real(fcg))\n",
    "\n",
    "#### and else >> \n",
    "#len(NEG_eeg)\n",
    "for i in tqdm.tqdm(range(1, len(NEG_eeg))):\n",
    "    try:\n",
    "        data = NEG_eeg[i]         \n",
    "        fcg = tvfcg(np.squeeze(data), estimator, fb, fs, cc, step)\n",
    "        NEG_fcgs = np.vstack([NEG_fcgs,np.array(np.real(fcg))])\n",
    "\n",
    "    except:\n",
    "        print(\"window is too large for this sentence\")\n",
    "        neg_err +=1\n",
    "\n",
    "\n",
    "print(\"...done!üéâ\")   \n",
    "print(\"NEG_fcgs = \", len(NEG_fcgs))\n",
    "print(\"Error sentence =\", neg_err)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(POS_fcgs.shape)\n",
    "#print(NEU_fcgs.shape)\n",
    "print(NEG_fcgs.shape)\n",
    "\n",
    "print(pos_err)\n",
    "#print(neu_err)\n",
    "print(neg_err)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#P/N/N dict\n",
    "#P/N/N fcgs\n",
    "\n",
    "#save_dir = \"/home/KimDY/zuco-nlp/sentiment-analysis/Data_to_preprocess/\"\n",
    "\n",
    "with open(\"POS_theta_fcgs.pkl\", \"wb\") as f:\n",
    "   pickle.dump(POS_fcgs, f)\n",
    "# with open(\"NEU_fcgs.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(NEU_fcgs, f)\n",
    "with open(\"NEG_theta_fcgs.pkl\", \"wb\") as f:\n",
    "    pickle.dump(NEG_fcgs, f)\n",
    "    \n",
    "# with open(\"POS_dict.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(POS_dict, f)\n",
    "# with open(\"NEU_dict.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(NEU_dict, f)\n",
    "# with open(\"NEG_dict.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(NEG_dict, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "import scipy\n",
    "from dyconnmap.fc import iplv\n",
    "import matplotlib.pyplot as plt\n",
    "from dyconnmap import tvfcg\n",
    "from dyconnmap.fc import IPLV\n",
    "from dyconnmap.ts import transition_rate, occupancy_time\n",
    "from dyconnmap.cluster import NeuralGas\n",
    "import tqdm\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"NEG_fcgs.pkl\", \"rb\") as f:\n",
    "    NEG_fcgs = pickle.load(f)\n",
    "    \n",
    "with open(\"POS_fcgs.pkl\", \"rb\") as f:\n",
    "    POS_fcgs = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_fcgs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. fcg stack and Neural gas fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 class (binary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fcgs_POS, _, _ = np.shape(POS_fcgs)\n",
    "num_fcgs_NEG, _, _ = np.shape(NEG_fcgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(POS_fcgs.shape)\n",
    "print(NEG_fcgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcgs = np.vstack([POS_fcgs, NEG_fcgs])\n",
    "print(len(fcgs))\n",
    "num_fcgs, num_channels, num_channels = np.shape(fcgs)\n",
    "\n",
    "triu_ind = np.triu_indices_from(np.squeeze(fcgs[0, ...]), k=1)\n",
    "\n",
    "fcgs = fcgs[:, triu_ind[0], triu_ind[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "# what \"N\" is best?\n",
    "mdl = NeuralGas(n_protos=10, rng=rng).fit(fcgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding, symbols = mdl.encode(fcgs)\n",
    "\n",
    "#Separate the encoded symbols based on their original groupings\n",
    "grp_dist_POS = symbols[0:num_fcgs_POS]\n",
    "grp_dist_NEG = symbols[num_fcgs_POS:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "with open(\"2class_mdl.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mdl, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CHECK BEFORE INSERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0. import \n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "import numpy as np\n",
    "import warnings\n",
    "import scipy\n",
    "from dyconnmap.fc import iplv\n",
    "import matplotlib.pyplot as plt\n",
    "from dyconnmap import tvfcg\n",
    "from dyconnmap.fc import IPLV\n",
    "from dyconnmap.ts import transition_rate, occupancy_time\n",
    "from dyconnmap.cluster import NeuralGas\n",
    "import tqdm\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#7. load mdl & set property\n",
    "with open(\"2class_mdl.pkl\", \"rb\") as f:\n",
    "    mdl = pickle.load(f)\n",
    "    \n",
    "fb = [4.0, 50.0] #lowest , Highest frequency band\n",
    "cc = 4.0\n",
    "fs = 500.0\n",
    "step = 250 #this time, we will slice 80 for window \n",
    "\n",
    "\n",
    "\n",
    "estimator = IPLV(fb, fs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#8. load EEG & Transpose EEG\n",
    "with open(\"POS_dict.pkl\", \"rb\") as f:\n",
    "    POS_dict = pickle.load(f)    \n",
    "POS_eeg = []\n",
    "\n",
    "for i in range(len(POS_dict)):\n",
    "    POS_eeg.append(POS_dict[i][\"RAW_EEG\"].T) #need to transpose\n",
    "\n",
    "#POS_eeg[0].shape == (105, 3412)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 105, 105)\n",
      "(12, 5460)\n"
     ]
    }
   ],
   "source": [
    "#9. calculate fcg\n",
    "eeg = POS_eeg[0] #1 Í∞úÏùò EEG \n",
    "fcg = tvfcg(np.squeeze(eeg), estimator, fb, fs, cc, step) #1_word_1_fix\n",
    "fcgs = np.array(np.real(fcg))\n",
    "\n",
    "print(fcgs.shape)\n",
    "#fcgs.shape = (12, 105, 105) <- ÌòÑÏû¨ ÌîºÌÅ¥ÌååÏùºÎ°ú Ï†ÄÏû•ÎêòÏñ¥ÏûàÎäî fcg ÌòïÌÉú. Î¨∏Ïû•Ïóê ÏÉÅÍ¥ÄÏóÜÏù¥ Í∑∏ÎÉ• Î™ΩÎïÖ Stack ÎêòÏñ¥ÏûàÏùå. ÏïûÏúºÎ°ú Ïì∏ÏùºÏóÜÏùÑÎìØ? \n",
    "\n",
    "triu_ind = np.triu_indices_from(np.squeeze(fcgs[0, ...]), k=1)\n",
    "fcgs = fcgs[:, triu_ind[0], triu_ind[1]]\n",
    "\n",
    "print(fcgs.shape)\n",
    "\n",
    "#fcgs.shape == (12, 5460) <- encode Ï†ÑÏóê ÎÑ£Ïñ¥Ïïº ÌïòÎäî fcg ÌòïÌÉú "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#10. Using mdl, encode \n",
    "encoding, symbols = mdl.encode(fcgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 1)\n",
      "(12, 1, 5460)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[0.06264571, 0.07973764, 0.09829245, ..., 0.04474551,\n",
       "         0.02467856, 0.03130361]],\n",
       "\n",
       "       [[0.04397912, 0.04797952, 0.05892048, ..., 0.04241011,\n",
       "         0.02207803, 0.02168266]],\n",
       "\n",
       "       [[0.04397912, 0.04797952, 0.05892048, ..., 0.04241011,\n",
       "         0.02207803, 0.02168266]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.05965754, 0.07291722, 0.11318607, ..., 0.04981235,\n",
       "         0.02560415, 0.02311476]],\n",
       "\n",
       "       [[0.05325136, 0.06283716, 0.06421158, ..., 0.03514461,\n",
       "         0.02386462, 0.02417941]],\n",
       "\n",
       "       [[0.06264571, 0.07973764, 0.09829245, ..., 0.04474551,\n",
       "         0.02467856, 0.03130361]]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(symbols.shape) #ÏùºÎã® Ïù¥Í≤å ÌõàÎ†® Î™®Îç∏Ïóê Îì§Ïñ¥Í∞ê...\n",
    "print(encoding.shape)\n",
    "\n",
    "symbols #fcg 12Ïû• Í∞ÅÍ∞Å 1Í∞úÏî© ÏûàÎäî brain state (label, int) Ï†ïÎ≥¥\n",
    "encoding # fcg 12Ïû• Í∞ÅÍ∞ÅÏóê 1Í∞úÏî© ÏûàÎäî brain state soft label Ï†ïÎ≥¥. Ïù¥Îïå soft labelÏùÄ Í∞Å Ï±ÑÎÑê(??)ÎßàÎã§ ÏûàÏùå.\n",
    "\n",
    "# Í≥†ÎØº1. symbols ÏùÑ ÎÑ£ÏùÑÏßÄ encoding ÏùÑ ÎÑ£ÏùÑÏßÄ?\n",
    "# Í≥†ÎØº2. Ïñ¥Îñ†Ìïú ÌòïÌÉúÎ°ú ÎÑ£Ïñ¥Ï§ÑÏßÄ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLUS analysis IN Jupyter_2Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_dist_POS = symbols[0:num_fcgs_POS]\n",
    "grp_dist_NEG = symbols[num_fcgs_POS:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_grp_dist_POS = np.histogram(grp_dist_POS, bins=mdl.n_protos, normed=True)\n",
    "h_grp_dist_NEG = np.histogram(grp_dist_NEG, bins=mdl.n_protos, normed=True)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "ind = np.arange(mdl.n_protos)\n",
    "p1 = ax.bar(ind - 0.125, h_grp_dist_POS[0], 0.25, label='POS')\n",
    "p2 = ax.bar(ind + 0.125, h_grp_dist_NEG[0], 0.25, label='NEG')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel('Symbol Index')\n",
    "ax.set_ylabel('Hits %')\n",
    "ax.set_xticks(np.arange(mdl.n_protos))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "protos_mtx = np.zeros((mdl.n_protos, 105, 105))\n",
    "\n",
    "for i in range(mdl.n_protos):\n",
    "    symbol_state = np.zeros((105, 105))\n",
    "    symbol_state[triu_ind] = mdl.protos[i, :]\n",
    "    symbol_state = symbol_state + symbol_state.T\n",
    "    np.fill_diagonal(symbol_state, 1.0)\n",
    "    \n",
    "    protos_mtx[i, :, :] = symbol_state\n",
    "    \n",
    "mtx_min = np.min(protos_mtx)\n",
    "mtx_max = np.max(protos_mtx)\n",
    "\n",
    "f, ax = plt.subplots(ncols=mdl.n_protos, figsize=(12, 12))\n",
    "for i in range(mdl.n_protos):\n",
    "    cax = ax[i].imshow(np.squeeze(protos_mtx[i,...]), vmin=mtx_min, vmax=mtx_max, cmap=plt.cm.Spectral)\n",
    "    ax[i].set_title('#{0}'.format(i))\n",
    "\n",
    "# move the colorbar to the side ;)\n",
    "f.subplots_adjust(right=0.8)\n",
    "cbar_ax = f.add_axes([0.82, 0.445, 0.0125, 0.115])\n",
    "cb = f.colorbar(cax, cax=cbar_ax)\n",
    "cb.set_label('Imaginary PLV')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grp_dist_POS.shape)\n",
    "print(grp_dist_NEG.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Examine the first subject\n",
    "\n",
    "subj1_POS = grp_dist_POS\n",
    "subj1_NEG = grp_dist_NEG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dyconnmap.ts import markov_matrix\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "markov_matrix_eo = markov_matrix(subj1_POS)\n",
    "markov_matrix_ec = markov_matrix(subj1_NEG)\n",
    "\n",
    "f = plt.figure(figsize=(8, 6))\n",
    "grid = ImageGrid(f, 111,\n",
    "                 nrows_ncols=(1,2),\n",
    "                 axes_pad=0.15,\n",
    "                 share_all=True,\n",
    "                 cbar_location=\"right\",\n",
    "                 cbar_mode=\"single\",\n",
    "                 cbar_size=\"7%\",\n",
    "                 cbar_pad=0.15,\n",
    "                 )\n",
    "im = grid[0].imshow(markov_matrix_eo, vmin=0.0, vmax=1.0, cmap=plt.cm.Spectral)\n",
    "grid[0].set_xlabel('Prototype')\n",
    "grid[0].set_ylabel('Prototype')\n",
    "grid[0].set_title('NR')\n",
    "\n",
    "im = grid[1].imshow(markov_matrix_ec, vmin=0.0, vmax=1.0, cmap=plt.cm.Spectral)\n",
    "grid[1].set_xlabel('Prototype')\n",
    "grid[1].set_ylabel('Prototype')\n",
    "grid[1].set_title('TSR')\n",
    "\n",
    "cb = grid[1].cax.colorbar(im)\n",
    "cax = grid.cbar_axes[0]\n",
    "axis = cax.axis[cax.orientation]\n",
    "axis.label.set_text(\"Transition Probability\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dyconnmap.ts import transition_rate, occupancy_time\n",
    "\n",
    "\n",
    "tr_eo = transition_rate(subj1_POS)\n",
    "tr_ec = transition_rate(subj1_NEG)\n",
    "\n",
    "print(f\"\"\"\n",
    "Transition rate\n",
    "===============\n",
    "  NR: {tr_eo:.3f}\n",
    "  TSR : {tr_ec:.3f}\n",
    "\"\"\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "occ_eo = occupancy_time(subj1_POS)[0]\n",
    "occ_ec = occupancy_time(subj1_NEG)[0]\n",
    "\n",
    "print(\"\"\"\n",
    "Occupancy time\n",
    "==============\n",
    "    State \\t 0 \\t 1 \\t 2 \\t 3 \\t 4\n",
    "      -----\n",
    "  NR:      \\t {0:.3f} \\t {1:.3f} \\t {2:.3f} \\t {3:.3f} \\t {4:.3f}\n",
    "  TSR : \\t {5:.3f} \\t {6:.3f} \\t {7:.3f} \\t {8:.3f} \\t {9:.3f}\n",
    "\"\"\".format(*occ_eo, *occ_ec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_fcgs_NEU, _, _ = np.shape(NEU_fcgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcgs = np.vstack([POS_fcgs, NEU_fcgs, NEG_fcgs]) #can?\n",
    "print(len(fcgs))\n",
    "num_fcgs, num_channels, num_channels = np.shape(fcgs)\n",
    "\n",
    "triu_ind = np.triu_indices_from(np.squeeze(fcgs[0, ...]), k=1)\n",
    "\n",
    "fcgs = fcgs[:, triu_ind[0], triu_ind[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "# what \"N\" is best?\n",
    "mdl = NeuralGas(n_protos=5, rng=rng).fit(fcgs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save\n",
    "with open(save_dir + \"3class_mdl.pkl\", \"wb\") as f:\n",
    "    pickle.dump(mdl, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding, symbols = mdl.encode(fcgs)\n",
    "\n",
    "#Separate the encoded symbols based on their original groupings\n",
    "grp_dist_POS = symbols[0:num_fcgs_POS]\n",
    "grp_dist_NEU = symbols[num_fcgs_POS:num_fcgs_NEU]\n",
    "grp_dist_NEG = symbols[num_fcgs_NEU:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLUS analysis IN Jupyter_3Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
